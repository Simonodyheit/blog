<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Research note | Simon&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.9.5">
    
    <meta name="description" content="Life is an odyssey.">
    
    <link rel="preload" href="/blog/assets/css/0.styles.399705c3.css" as="style"><link rel="preload" href="/blog/assets/js/app.c49cc318.js" as="script"><link rel="preload" href="/blog/assets/js/3.daebfe5d.js" as="script"><link rel="preload" href="/blog/assets/js/1.f51c7ee6.js" as="script"><link rel="preload" href="/blog/assets/js/22.38fbc5ab.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.dcced57f.js"><link rel="prefetch" href="/blog/assets/js/11.c2400523.js"><link rel="prefetch" href="/blog/assets/js/12.daf48828.js"><link rel="prefetch" href="/blog/assets/js/13.73a5a540.js"><link rel="prefetch" href="/blog/assets/js/14.ba9573fb.js"><link rel="prefetch" href="/blog/assets/js/15.a1282f11.js"><link rel="prefetch" href="/blog/assets/js/16.a547dfde.js"><link rel="prefetch" href="/blog/assets/js/17.4a3d807e.js"><link rel="prefetch" href="/blog/assets/js/18.56bbb100.js"><link rel="prefetch" href="/blog/assets/js/19.5c520a59.js"><link rel="prefetch" href="/blog/assets/js/20.617bcc46.js"><link rel="prefetch" href="/blog/assets/js/21.ccca0304.js"><link rel="prefetch" href="/blog/assets/js/23.3cdbcc0c.js"><link rel="prefetch" href="/blog/assets/js/24.80b8a5ba.js"><link rel="prefetch" href="/blog/assets/js/25.50ec16b1.js"><link rel="prefetch" href="/blog/assets/js/26.2d06dfbc.js"><link rel="prefetch" href="/blog/assets/js/27.4a9e288b.js"><link rel="prefetch" href="/blog/assets/js/28.d10cc2d5.js"><link rel="prefetch" href="/blog/assets/js/29.a740c8d9.js"><link rel="prefetch" href="/blog/assets/js/30.d32d2e5c.js"><link rel="prefetch" href="/blog/assets/js/4.679cc3e5.js"><link rel="prefetch" href="/blog/assets/js/5.458a4984.js"><link rel="prefetch" href="/blog/assets/js/6.7a8d9ec1.js"><link rel="prefetch" href="/blog/assets/js/7.0ebbc06f.js"><link rel="prefetch" href="/blog/assets/js/8.1957ba5a.js"><link rel="prefetch" href="/blog/assets/js/9.31fb02b0.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.399705c3.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1aefc0b4><div data-v-1aefc0b4><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1aefc0b4 data-v-1aefc0b4><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Simon's Blog</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2>Life is an odyssey.</p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><!---->
            
          <!---->
          2022
        </a></span></div></div> <div class="hide" data-v-1aefc0b4><header class="navbar" data-v-1aefc0b4><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.jpg" alt="Simon's Blog" class="logo"> <span class="site-name">Simon's Blog</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/post/" class="nav-link"><i class="iconfont reco-blog"></i>
  Post
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/notes/" class="nav-link router-link-active"><i class="undefined"></i>
  Note
</a></li><li class="dropdown-item"><!----> <a href="/blog/roadmap/" class="nav-link"><i class="undefined"></i>
  Roadmap
</a></li><li class="dropdown-item"><!----> <a href="/blog/resource/" class="nav-link"><i class="undefined"></i>
  Resource
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      About Me
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/Simonodyheit" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://leetcode.cn/u/simon-zhang-1/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-api"></i>
  LeetCode
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1aefc0b4></div> <aside class="sidebar" data-v-1aefc0b4><div class="personal-info-wrapper" data-v-39576ba9 data-v-1aefc0b4><!----> <!----> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>18</h3> <h6 data-v-39576ba9>Articles</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>12</h3> <h6 data-v-39576ba9>Tags</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/post/" class="nav-link"><i class="iconfont reco-blog"></i>
  Post
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/notes/" class="nav-link router-link-active"><i class="undefined"></i>
  Note
</a></li><li class="dropdown-item"><!----> <a href="/blog/roadmap/" class="nav-link"><i class="undefined"></i>
  Roadmap
</a></li><li class="dropdown-item"><!----> <a href="/blog/resource/" class="nav-link"><i class="undefined"></i>
  Resource
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="undefined"></i>
      About Me
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/Simonodyheit" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="https://leetcode.cn/u/simon-zhang-1/" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-api"></i>
  LeetCode
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Research note</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><!---->
            
          <!---->
          2022
        </a></span></div></div> <div data-v-1aefc0b4><main class="page"><section><div class="page-title"><h1 class="title">Research note</h1> <div data-v-f875f3fc><!----> <!----> <!----> <i class="tags iconfont reco-tag" data-v-f875f3fc><span class="tag-item" data-v-f875f3fc>note</span></i></div></div> <div class="theme-reco-content content__default"><p>This is a new notebook. Why not start it all in the VSCode here. The local mode, yep.</p> <h3 id="_12-12-2021"><a href="#_12-12-2021" class="header-anchor">#</a> 12/12/2021</h3> <p>FYI, We haven't much time left for this year.</p> <h3 id="_12-13-2021"><a href="#_12-13-2021" class="header-anchor">#</a> 12/13/2021</h3> <p>Here are some <strong>points</strong> to make:<br>
Gated; local and global; fixed and dynamical; local and long-range; Mono and dual; coarse and fine;
Uncertainty-Aware</p> <h3 id="_12-14-2021"><a href="#_12-14-2021" class="header-anchor">#</a> 12/14/2021</h3> <ul><li>[x] Let's start with the basic retinal vessel segmentation. (https://github.com/lee-zq/VesselSeg-Pytorch) ---&gt; A landmark event. 😏<div class="language- extra-class"><pre class="language-text"><code>/mnt/data3/zzx/Dataset/Retinal
├── CHASEDB1
├── datasets.rar
├── DRIVE
└── STARE
</code></pre></div></li></ul> <h3 id="_12-15-2021"><a href="#_12-15-2021" class="header-anchor">#</a> 12/15/2021</h3> <p>It's better to not start over, but to build on others' work, to furnish that, to make it solid. Transfer? Let's set it aside for a while; we are just not that eager to do this.</p> <h3 id="_12-16-2021"><a href="#_12-16-2021" class="header-anchor">#</a> 12/16/2021</h3> <ul><li>[x] check the test ---&gt; Bingo. Now adapatable to any test dataset.</li> <li>commands in bash file are run sequentially, if anything wrong, execute the next one.</li> <li>attention to lines when writing to a file  <code>f.write('\n)</code></li></ul> <h3 id="_12-17-2021"><a href="#_12-17-2021" class="header-anchor">#</a> 12/17/2021</h3> <ul><li>[x] Start to check the MICCAI papers. (https://github.com/JunMa11/MICCAI-OpenSourcePapers/)</li></ul> <h3 id="_12-20-2021"><a href="#_12-20-2021" class="header-anchor">#</a> 12/20/2021</h3> <p>So basically did nothing today, except for full rest, fine meal, and great journey dining out.<br>
Still, we've got something to do. That's to learn something. The IELTS listening practice, the model transferring, etc...
**We definitely need a task-specific module for a given task, or why not make it more general? This can be hard, of course. Microvessels, edges, those are what we almost cared about, actually. **</p> <ul><li>[x] At first, try to transfer the polyp segmentation method onto the retinal vessel segmentation.</li></ul> <h3 id="_12-22-2021"><a href="#_12-22-2021" class="header-anchor">#</a> 12/22/2021</h3> <p>Succeed to integrate the CCBANet to current framework, 進める！ ---&gt; A landmark event. 😏<br>
We've seen a boost on the metrics.</p> <h3 id="_12-24-2021"><a href="#_12-24-2021" class="header-anchor">#</a> 12/24/2021</h3> <p>Run the following, remember to check them:</p> <ul><li>[x] <code>CUDA_VISIBLE_DEVICES=1 python -W ignore train.py --model_type SPiN --data_type DRIVE --loss_type spin_loss --N_epochs 69 --classes 2 # ! seems to be really faster and more precise?</code> ---&gt; 1640317517, 1640326687</li> <li>[X] <code>CUDA_VISIBLE_DEVICES=0 python test.py --random True --test_data_type DRIVE --config_path /mnt/data3/zzx/Results/ReVeSeg/DRIVE_KiUnet/1640247960 # 2021/12/24</code> ---&gt; 1640247960</li></ul> <hr> <h3 id="_12-26-2021"><a href="#_12-26-2021" class="header-anchor">#</a> 12/26/2021</h3> <p>Discovery today:</p> <ol><li>Models | Performance | Speed
<ol><li>DenseNet |</li> <li>LadderNet |</li> <li>CCBANet |</li> <li>KiUnet | | Very slow</li> <li>SPiNet | Not as good as expected without finetuning | Really fast</li></ol></li></ol> <h3 id="_12-27-2021"><a href="#_12-27-2021" class="header-anchor">#</a> 12/27/2021</h3> <ol><li>Go on reading papers</li></ol> <h3 id="_12-28-2021"><a href="#_12-28-2021" class="header-anchor">#</a> 12/28/2021</h3> <ol><li>loss choice in ReVeSeg.
<blockquote><p>However, the objects in medical images such as optic disc and retinal vessels often occupy a small region in the image. The cross entropy loss is not optimal for such tasks. In this paper, we use the Dice coefficient loss function [57], [58] to replace the common cross entropy loss.</p></blockquote></li></ol> <h3 id="_12-30-2021"><a href="#_12-30-2021" class="header-anchor">#</a> 12/30/2021</h3> <p>Wait! Spin!!! This seems still not really bad. Just ran another training for Spin. (1640837519, tmux7)
SANet can segment to some extent, but we need to verify it a little bit. Untested for the latest one.</p> <ul><li>[X] modify the SANet -----&gt; no good<br>
Just ran a UNet for comparison.</li></ul> <h3 id="_12-31-2021"><a href="#_12-31-2021" class="header-anchor">#</a> 12/31/2021</h3> <p>Weird, why UNet won't work here?</p> <h3 id="_01-01-2022"><a href="#_01-01-2022" class="header-anchor">#</a> 01/01/2022</h3> <p>Changed the loss function, then it works quite well.<br>
Does this mean my modification on the SANet has little to no effect?... Oops...</p> <h3 id="_01-03-2022"><a href="#_01-03-2022" class="header-anchor">#</a> 01/03/2022</h3> <p>Go on papers!</p> <h3 id="_01-04-2022"><a href="#_01-04-2022" class="header-anchor">#</a> 01/04/2022</h3> <ul><li>[X] Just transfered the SANet (Spatial Attention Network) from Keras to Pytorch.</li> <li>[X] <strong>Untrained. No CPU yet.</strong></li> <li>[X] Paper: EAR-NET: Error Attention Refining Network ForRetinal Vessel Segmentation <a href="https://github.com/Markin-Wang/EARNet" target="_blank" rel="noopener noreferrer">Github<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>[X] Implement the famous apex</li></ul> <h3 id="_01-05-2022"><a href="#_01-05-2022" class="header-anchor">#</a> 01/05/2022</h3> <ul><li>[X] <s>Go on on the EAR-NET</s></li> <li>[ ] runx: an experiment managemer # TODO</li> <li>[X] Just implemented the depth2space/depth2space to replace the pooling function. That's <code>SA_Net_v2</code>. Should I use any guidance to choose which neighbors just as in the paper (subpixel embedding?)</li></ul> <p>$\color{red}\text{Status Quo}$:<br>
Because of no GPU for now:</p> <ol><li>SA_Net, the transfered Pytorch version from Keras one, remains un-verified. Does it still have the potential to reach the state-of-the-art?  ------&gt; <code>base</code> (violance of naming ethnics)
<ul><li>[ ] Remained to be tested</li></ul></li> <li>SA_Net_v2, the replace-all-maxpooling-and-transposeconv-with-s2d-or-d2s version, remains un-verified. ------&gt; <code>sa2</code> <ul><li>[ ] Remained to be tested</li></ul></li> <li>SA_Net_v3, the not-replacement-but-guide-in-different-way version, remains un-verified.  -----&gt; <code>sa3</code> <ul><li>[ ] Remained to be tested</li></ul></li> <li>An Unet to check if everything works -----&gt; <code>unet</code> yes, things do work the same way as before
<ul><li>[ ] Remained to be tested</li></ul></li> <li>A multiplication-fusion-mode version of SA_Net_v3 -----&gt; <code>sa3_mul</code><br> <strong>Result</strong>:</li> <li>They do decrese to some extent, but just too slow and vibrating. Or it's just the formal models tend to over-fit while they reflect the true thing?</li></ol> <h3 id="_01-06-2022"><a href="#_01-06-2022" class="header-anchor">#</a> 01/06/2022</h3> <p>Why not just did everything on the UNet?</p> <ul><li>[ ]  Start to implement the same idea on the UNet. There, again, should be <strong>three</strong> versions.</li> <li>two training processes are going on.
<ul><li><code>/mnt/data3/zzx/Results/ReVeSeg/DRIVE_UNet_v3/1641460260/</code></li> <li><code>/mnt/data3/zzx/Results/ReVeSeg/DRIVE_UNet_v2/1641460871/</code></li></ul></li></ul> <ol><li>The Dropout seems to be of no use. We'll see if the above two can bring etwas gut. Hoffentlich.</li></ol> <ul><li>[X] PPT.</li></ul> <h3 id="_01-07-2022"><a href="#_01-07-2022" class="header-anchor">#</a> 01/07/2022</h3> <ul><li>[ ] Learning rate issue remains to be solved. Should not always increase.</li> <li>[ ] Pool the other way: Please immediately implement them to check if there is any difference, though I do think they perform the same way...
<ol><li><strong>Strategy1:</strong></li></ol> <div class="language-python extra-class"><pre class="language-python"><code>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channel<span class="token punctuation">,</span> out_channel<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre></div><ol start="2"><li><strong>Strategy2:</strong></li></ol> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DynamicPooling</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channel<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>s2d <span class="token operator">=</span> SpaceToDepth<span class="token punctuation">(</span><span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>maxpool <span class="token operator">=</span> Max_pool<span class="token punctuation">(</span><span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> Avg_pool<span class="token punctuation">(</span><span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>concat <span class="token operator">=</span> Concatenate<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> conv_block<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> channel<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> channel<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># import ipdb; ipdb.set_trace()</span>
        deep_x <span class="token operator">=</span> self<span class="token punctuation">.</span>s2d<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># channel ----&gt; 4 * channel</span>
        max_x <span class="token operator">=</span> self<span class="token punctuation">.</span>maxpool<span class="token punctuation">(</span>deep_x<span class="token punctuation">)</span>
        avg_x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>deep_x<span class="token punctuation">)</span>
        con <span class="token operator">=</span> self<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>max_x<span class="token punctuation">,</span> avg_x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>con<span class="token punctuation">)</span>
        weighted_x_deep <span class="token operator">=</span> out <span class="token operator">*</span> deep_x
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>weighted_x_deep<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">4</span> <span class="token comment"># sum</span>
        <span class="token keyword">return</span> out
</code></pre></div><ol start="3"><li><strong>Search if any previous works</strong></li></ol></li> <li>[ ] And agian, how the s2d/d2s really works?</li></ul> <blockquote><p>Controversially, Geoff Hinton doesn’t like Max-Pooling:
The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.</p></blockquote> <h2 id="paper-list"><a href="#paper-list" class="header-anchor">#</a> Paper list</h2> <p>Auch, we have MICCAI21 paper list for <strong>medical image segmentation</strong>.</p> <ol><li>A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation</li> <li>Anatomy-Constrained Contrastive Learning for Synthetic Segmentation without Ground-truth</li> <li>ASC-Net: Adversarial-based Selective Network for Unsupervised Anomaly Segmentation</li> <li><ul><li>[x] Automatic Polyp Segmentation via Multi-scale Subtraction Network</li></ul></li> <li><ul><li>[x] CCBANet: Cascading Context and Balancing Attention for Polyp Segmentation</li></ul></li> <li>Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation</li> <li><s>HRENet: A Hard Region Enhancement Network for Polyp Segmentation</s></li> <li><s>Hybrid graph convolutional neural networks for anatomical segmentation</s></li> <li>MT-UDA: Towards Unsupervised Cross-Modality Medical Image Segmentation with Limited Source Labels</li> <li><s>Multi-Compound Transformer for Accurate Biomedical Image Segmentation</s></li> <li>OLVA: Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation</li> <li><s>RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs using a Novel Multi-scale Generative Adversarial Network</s></li> <li><ul><li>[x] Shallow Attention Network for Polyp Segmentation</li></ul></li> <li><s>Towards Robust General Medical Image Segmentation</s></li> <li>TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</li> <li>Unsupervised Network Learning for Cell Segmentation</li></ol> <p>MICCAI20 paper list for <strong>medical image segmentation</strong></p> <ol><li><s>Source-Relaxed Domain Adaptation for Image Segmentation</s></li> <li>iU-Net: Towards Accurate Segmentation of Biomedical Images Using Over-Complete Representations</li> <li>Self-supervised Nuclei Segmentation in Histopathological Images Using Attention</li> <li>Adaptive Context Selection for Polyp Segmentation</li> <li>PraNet: Parallel Reverse Attention Network for Polyp Segmentation</li> <li>White Matter Tract Segmentation with Self-supervised Learning</li></ol> <p>MICCAI19 paper list for <strong>medical image segmentation</strong></p> <div class="language- extra-class"><pre class="language-text"><code>1. Boundary and Entropy-Driven Adversarial Learning for Fundus Image Segmentation
2. Selective Feature Aggregation Network with Area-Boundary Constraints for Polyp Segmentation
3. ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation
4. PHiSeg: Capturing Uncertainty in Medical Image Segmentation
5. Constrained Domain Adaptation for Segmentation
6. Refined Segmentation R-CNN: A Two-Stage Convolutional Neural Network for Punctate White Matter Lesion Segmentation in Preterm Infants
</code></pre></div><p>Others:</p> <div class="language- extra-class"><pre class="language-text"><code>1. Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging
2. Patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance
3. Self-supervised visual representation learning for histopathological images
4. Self-Supervised Vessel Enhancement Using Flow-Based Consistencies
Not finished...
</code></pre></div><hr> <h2 id="paper-note"><a href="#paper-note" class="header-anchor">#</a> Paper note</h2> <details><summary>Automatic Polyp Segmentation via Multi-scale Subtraction Network</summary> <p align="left">
Insight: (https://github.com/Xiaoqi-Zhao-DLUT/MSNet)
    </p><p>1. multi-scale subtraction network with multi-level and multi-stage cascaded subtraction operations
    </p> <p>2. LossNet. a training-free loss network to implement detail-to-structure supervision
</p></details> <hr> <details><summary>CCBANet: Cascading Context and Balancing Attention for Polyp Segmentation</summary> <p align="left">
Insight: (https://github.com/ntcongvn/CCBANet)
    </p><p>1. Balancing Attention Module. The BAM module implements separately and balances the attention mechanism for three regions: background, foreground, and the boundary site.
    </p>
Thought:
    <p> 1. Should we consider the background attention? It seems quite important to cover this in the attention mechanism, and it has valid implementation background. Seemingly a fairly well module in the decoder?
    </p> <p> 2. The boundary seems quite tricky, isn't that using a simple calculation too simple? Ich weiss es nicht. 
    </p> <p> 3. MIX IT UP! What if there is only one module in the skip connection path, based on the fact that I change the backbone to transformer? (Long range issue resolved inherently) 
    </p><p></p></details> <hr> <details><summary>Adaptive Context Selection for Polyp Segmentation</summary> <p align="left">
Insight: (https://github.com/ReaFly/ACSNet)
    </p><p>1. Local Context Attention Module (LCA). A kind of spatial attention scheme which attends more on the `local` part.
    </p> <p>2. Global Context Module (GCM). Four branches to extract features at different scales, capturing the long-range dependency while maintaining the original resolution.
    </p> <p>3. Adaptive Selection Module (ASM). Aiming to adaptively select context feature for aggregation in each block. Plus incorporation with a SE block to adaptively recalibrate channel-wise feature responses for feature selection.
    </p>
Thought:
    <p> 1. Can I use SwinTransformer as the backbone, and then focus on the local context feature in decoder?
    </p> <p> 2. There seems to be a multi-level manner in the decoder? 
    </p> <p></p></details> <hr> <details><summary>Shallow Attention Network for Polyp Segmentation</summary> <p align="left">
Issue overhead:
    </p><p> 1. inconsistent colors among samples
    </p> <p> 2. feature downsampling leads to degraded small polyps
    </p> <p> 3. foreground and background pixels are imbalanced, leading to biased training
    </p>
Insight: (https://github.com/weijun88/SANet)
    <p> 1. to enhance the segmentation quality of small polyps, shallow attention module is used to filter out the background noise of shallow features (What is shallow features?)  
    </p> <p> 2. to erase the severe pixel imbalance for small polyps, a probability correction strategy is proposed
    </p> <p> 3. small polyp segmentation relies more on shallow features, because they have higher resolutions and contain richer details compared with deep ones.  But the shallow features are too noisy to be used directly.
    </p> <p> 4. <font color="red"><b>PCS</b></font></p>
Thought:
    <p> 1. In retinal vessel segmentation task also exists such issues. Can we transfer that to the problem? inconsistent colors, imbalanced ratio of marco and micro vessels.
    </p> <p> 2. 🙂 The rationale behind the Insight3 also holds true for ReVe segmentation task. 
    </p> <p> 3. <b>Enhancement of features</b></p> <p></p></details> <div class="language- extra-class"><pre class="language-text"><code>def save_prediction(self):
        print(self.args.datapath.split('/')[-1])
        with torch.no_grad():
            for image, mask, shape, name, origin in self.loader:
                image = image.cuda().float()
                pred  = self.model(image)
                pred  = F.interpolate(pred, size=shape, mode='bilinear', align_corners=True)[0, 0]
                pred[torch.where(pred&gt;0)] /= (pred&gt;0).float().mean()
                pred[torch.where(pred&lt;0)] /= (pred&lt;0).float().mean()
                pred  = torch.sigmoid(pred).cpu().numpy()*255
                if not os.path.exists(self.args.predpath):
                    os.makedirs(self.args.predpath)
                cv2.imwrite(self.args.predpath+'/'+name[0], np.round(pred))
</code></pre></div><details><summary>KiU-Net: Towards Accurate Segmentation of Biomedical Images Using Over-Complete Representations</summary> <p align="left">
Insight:
</p><p> 1. The increasing receptive field reasons is critical for CNNs to learn high-level features like objects, shapes or blobs. However, a side effect is that it reduces the focus of the filters. That is, except the first layer, filters in the other layers have reduced abilities to learn features that correspond to fine details like edges and their texture, which causes any network with the standard under-complete architecture to not produce sharp perdictions around the edges in tasks like segmentation.
</p> <p> 2. Hence, by constricting the receptive field size, filters in the deep layers can still learn very fine edges as it tries to focus heavily on smaller regions.
</p> <p> 3. Cross Resudual Fusion Module. One can perform simple concatenation of features at the final layer, however, this may not be necessarily optimal. Instead, feature maps are combined at each block, which results in better convergence as the flow of gradients during back propagation is across both the branches at each block level.</p>
Thought:
<p> 1. See the Insight1? In the MICCAI21, we can see that the SANet attempts to address such issue by introducing the shallow attention module. Especially for retinal vessel segmentation, we can say that fine details such as edges, mirco-vessels are foremost important for ReVeSeg.</p> <p> 2. Auch, local, local and local!</p> <p></p></details> <details><summary>Small Lesion Segmentation in Brain MRIs with Subpixel Embedding</summary> <p align="left">
Insight:
</p><p> 1. Subpixel embedding. We proposed to retain small local structures by learning an <b>embedding</b> that maps the input to high dimensional feature maps of twice the input resolution. Unlike typical CNNs, such operation do not perform lossy downsampling on the representation, hence, <b>the embedding preserves local structures, but lacks global context</b>.</p> <p></p></details> <details><summary>Joint Segment-level and Pixel-Wise Losses for Deep Learning Based Retinal Vessel Segmentation</summary> <p align="left">
Insight:
</p><p> 1. Deep learning based methods for retinal vessel segmentation are usually trained based on pixel-wise losses, which treat all vessel pixels with equal importance in pixel-to-pixel matching between a predicted probability map and the corresponding mannually annotated segmentation. However, due to the highly imbalanced pixel ratio between thick and thin vessels in fundus images, a pixel-wise loss would limit deep learning models to learn features for accurate segmentation of thin vessels. (exactly what I thought backwards) </p> <p> 2. Greater thickness inconsistency of thin vessels is due to the dominant amount of thick vessel pixels which makes the optimization of segmentation results largely influenced by the thick vessels.</p> <p> 3. threshold-free vessel segmentation
we apply the Otsu algorithm to automatically convert the probability map into the hard segmentation map.
</p>
Thought:
<p> 1. What about thinness attention? local attention? </p> <p> 2. Can hard threshold be soft? Yes.</p> <p> 3. </p> <p></p></details> <details><summary>NFN+: A novel network followed network for retinal vessel segmentation</summary> <p align="left">
Insight:
</p><p> 1. The difficulties are mainly caused by intensity inhomogeneity, low contrast, and variable thickness between capillaries and major vessels on a fundus retinal image. The retinal lesions and exudates can further complicate the segmentation.</p> <p> 2. Inter-vessel differences. Although the pooling layer used in DCNNs decreases the computational complexity, the down-sampling process would remove the fine structures such as capillaries, which can hardly be recovered by the up-sampling layers in the decoder.</p> <p> 3. Lack of structured prediction. Although DCNNs have a strong ability to learn image representations, they can hardly incorporate the spatial information of target regions into the unified image representations and pixel classification architecture. ...there is still an inherent contradictory between the pixel-level DCNNs and semantic-level structured prediction tasks. </p> <p> 4. The front network converts an image patch into a probalistic retinal vessel map, and the followed network further refines the map into a segmentation result. 
</p> <p> 5. Similar to giving those students with poor grades a chance to review and have a make-up examination, we use the followed network to learn the discrimination of vessels and background again based on the features and predictions produced by the front network, to correct some of those mis-classified pixels, particularly those with a probability close to the cut-off.  
</p> <p> 6. The image segmentation process can be regarded as a transfer route from the original image domain to the segmentation target domain. Such abstractions, however, may cause the loss of useful semantic and spatial information stage by stage. (One way is to use skip connection, another way is to use other down-sampling operations, such as space2depth and depth2space?) 
</p>
Thought:
<p> 1. What exactly is <b>multi-scale</b></p> <p> 2. coarse-to-fine processing and gradual abstraction</p> <p> 3. Overall, this paper offer us information about what the challenges are and why the challenges happen. </p> <p></p></details> <details><summary>SCS-Net: A Scale and Context Sensitive Network for Retinal Vessel Segmentation</summary> <p align="left">
Insight:
</p><p> 1. An intuitive idea is that the semantic information in the high-level features and the spatial information in the low-level features can be fully combined while suppressing mimics and noise.</p> <p> 2. </p> <p> 3.</p>
Thought:
<p> 1.</p> <p> 2.</p> <p> 3.</p> <p></p></details> <details><summary>EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation</summary> <p align="left">
Insight:
</p><p> 1. One major problem in the deep learning based models defecting the retinal blood vessel segmentation performance is that they normally recover the high-resolution representation from the low-resolution features. Essential information may be lost during this phase, causing inaccurate results.</p> <p> 2. Besides, most existing methods struggle to handle the pixel around the brighter and darker spots, leading to the false prediction.</p> <p> 3. <b>Moreover, most of existing works are dominated by the background due to the significant imbalance between the foreground and background, leading to high accuracy and specificity but low sensitivity.</b></p>
Thought:
<p> 1.</p> <p> 2.</p> <p> 3.</p> <p></p></details></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-12-2021" class="sidebar-link reco-side-_12-12-2021" data-v-cb1513f6>12/12/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-13-2021" class="sidebar-link reco-side-_12-13-2021" data-v-cb1513f6>12/13/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-14-2021" class="sidebar-link reco-side-_12-14-2021" data-v-cb1513f6>12/14/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-15-2021" class="sidebar-link reco-side-_12-15-2021" data-v-cb1513f6>12/15/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-16-2021" class="sidebar-link reco-side-_12-16-2021" data-v-cb1513f6>12/16/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-17-2021" class="sidebar-link reco-side-_12-17-2021" data-v-cb1513f6>12/17/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-20-2021" class="sidebar-link reco-side-_12-20-2021" data-v-cb1513f6>12/20/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-22-2021" class="sidebar-link reco-side-_12-22-2021" data-v-cb1513f6>12/22/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-24-2021" class="sidebar-link reco-side-_12-24-2021" data-v-cb1513f6>12/24/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-26-2021" class="sidebar-link reco-side-_12-26-2021" data-v-cb1513f6>12/26/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-27-2021" class="sidebar-link reco-side-_12-27-2021" data-v-cb1513f6>12/27/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-28-2021" class="sidebar-link reco-side-_12-28-2021" data-v-cb1513f6>12/28/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-30-2021" class="sidebar-link reco-side-_12-30-2021" data-v-cb1513f6>12/30/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_12-31-2021" class="sidebar-link reco-side-_12-31-2021" data-v-cb1513f6>12/31/2021</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_01-01-2022" class="sidebar-link reco-side-_01-01-2022" data-v-cb1513f6>01/01/2022</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_01-03-2022" class="sidebar-link reco-side-_01-03-2022" data-v-cb1513f6>01/03/2022</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_01-04-2022" class="sidebar-link reco-side-_01-04-2022" data-v-cb1513f6>01/04/2022</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_01-05-2022" class="sidebar-link reco-side-_01-05-2022" data-v-cb1513f6>01/05/2022</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_01-06-2022" class="sidebar-link reco-side-_01-06-2022" data-v-cb1513f6>01/06/2022</a></li><li class="level-3" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#_01-07-2022" class="sidebar-link reco-side-_01-07-2022" data-v-cb1513f6>01/07/2022</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#paper-list" class="sidebar-link reco-side-paper-list" data-v-cb1513f6>Paper list</a></li><li class="level-2" data-v-cb1513f6><a href="/blog/notes/Research/paper_notes.html#paper-note" class="sidebar-link reco-side-paper-note" data-v-cb1513f6>Paper note</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><div id="live2d-widget" class="live2d-widget-container" style="position:fixed;right:-30px;bottom:-60px;width:300px;height:600px;z-index:99999;opacity:0.5;pointer-events:none;"><canvas id="live2d_canvas" width="300" height="600" class="live2d_canvas" style="position:absolute;left:0px;top:0px;width:300px;height:600px;"></canvas></div></div></div>
    <script src="/blog/assets/js/app.c49cc318.js" defer></script><script src="/blog/assets/js/3.daebfe5d.js" defer></script><script src="/blog/assets/js/1.f51c7ee6.js" defer></script><script src="/blog/assets/js/22.38fbc5ab.js" defer></script>
  </body>
</html>
