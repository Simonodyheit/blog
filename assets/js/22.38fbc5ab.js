(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{645:function(e,t,a){"use strict";a.r(t);var n=a(16),s=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("This is a new notebook. Why not start it all in the VSCode here. The local mode, yep.")]),e._v(" "),a("h3",{attrs:{id:"_12-12-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-12-2021"}},[e._v("#")]),e._v(" 12/12/2021")]),e._v(" "),a("p",[e._v("FYI, We haven't much time left for this year.")]),e._v(" "),a("h3",{attrs:{id:"_12-13-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-13-2021"}},[e._v("#")]),e._v(" 12/13/2021")]),e._v(" "),a("p",[e._v("Here are some "),a("strong",[e._v("points")]),e._v(" to make:"),a("br"),e._v("\nGated; local and global; fixed and dynamical; local and long-range; Mono and dual; coarse and fine;\nUncertainty-Aware")]),e._v(" "),a("h3",{attrs:{id:"_12-14-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-14-2021"}},[e._v("#")]),e._v(" 12/14/2021")]),e._v(" "),a("ul",[a("li",[e._v("[x] Let's start with the basic retinal vessel segmentation. (https://github.com/lee-zq/VesselSeg-Pytorch) ---\x3e A landmark event. ðŸ˜"),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("/mnt/data3/zzx/Dataset/Retinal\nâ”œâ”€â”€ CHASEDB1\nâ”œâ”€â”€ datasets.rar\nâ”œâ”€â”€ DRIVE\nâ””â”€â”€ STARE\n")])])])])]),e._v(" "),a("h3",{attrs:{id:"_12-15-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-15-2021"}},[e._v("#")]),e._v(" 12/15/2021")]),e._v(" "),a("p",[e._v("It's better to not start over, but to build on others' work, to furnish that, to make it solid. Transfer? Let's set it aside for a while; we are just not that eager to do this.")]),e._v(" "),a("h3",{attrs:{id:"_12-16-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-16-2021"}},[e._v("#")]),e._v(" 12/16/2021")]),e._v(" "),a("ul",[a("li",[e._v("[x] check the test ---\x3e Bingo. Now adapatable to any test dataset.")]),e._v(" "),a("li",[e._v("commands in bash file are run sequentially, if anything wrong, execute the next one.")]),e._v(" "),a("li",[e._v("attention to lines when writing to a file  "),a("code",[e._v("f.write('\\n)")])])]),e._v(" "),a("h3",{attrs:{id:"_12-17-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-17-2021"}},[e._v("#")]),e._v(" 12/17/2021")]),e._v(" "),a("ul",[a("li",[e._v("[x] Start to check the MICCAI papers. (https://github.com/JunMa11/MICCAI-OpenSourcePapers/)")])]),e._v(" "),a("h3",{attrs:{id:"_12-20-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-20-2021"}},[e._v("#")]),e._v(" 12/20/2021")]),e._v(" "),a("p",[e._v("So basically did nothing today, except for full rest, fine meal, and great journey dining out."),a("br"),e._v("\nStill, we've got something to do. That's to learn something. The IELTS listening practice, the model transferring, etc...\n**We definitely need a task-specific module for a given task, or why not make it more general? This can be hard, of course. Microvessels, edges, those are what we almost cared about, actually. **")]),e._v(" "),a("ul",[a("li",[e._v("[x] At first, try to transfer the polyp segmentation method onto the retinal vessel segmentation.")])]),e._v(" "),a("h3",{attrs:{id:"_12-22-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-22-2021"}},[e._v("#")]),e._v(" 12/22/2021")]),e._v(" "),a("p",[e._v("Succeed to integrate the CCBANet to current framework, é€²ã‚ã‚‹ï¼ ---\x3e A landmark event. ðŸ˜"),a("br"),e._v("\nWe've seen a boost on the metrics.")]),e._v(" "),a("h3",{attrs:{id:"_12-24-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-24-2021"}},[e._v("#")]),e._v(" 12/24/2021")]),e._v(" "),a("p",[e._v("Run the following, remember to check them:")]),e._v(" "),a("ul",[a("li",[e._v("[x] "),a("code",[e._v("CUDA_VISIBLE_DEVICES=1 python -W ignore train.py --model_type SPiN --data_type DRIVE --loss_type spin_loss --N_epochs 69 --classes 2 # ! seems to be really faster and more precise?")]),e._v(" ---\x3e 1640317517, 1640326687")]),e._v(" "),a("li",[e._v("[X] "),a("code",[e._v("CUDA_VISIBLE_DEVICES=0 python test.py --random True --test_data_type DRIVE --config_path /mnt/data3/zzx/Results/ReVeSeg/DRIVE_KiUnet/1640247960 # 2021/12/24")]),e._v(" ---\x3e 1640247960")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_12-26-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-26-2021"}},[e._v("#")]),e._v(" 12/26/2021")]),e._v(" "),a("p",[e._v("Discovery today:")]),e._v(" "),a("ol",[a("li",[e._v("Models | Performance | Speed\n"),a("ol",[a("li",[e._v("DenseNet |")]),e._v(" "),a("li",[e._v("LadderNet |")]),e._v(" "),a("li",[e._v("CCBANet |")]),e._v(" "),a("li",[e._v("KiUnet | | Very slow")]),e._v(" "),a("li",[e._v("SPiNet | Not as good as expected without finetuning | Really fast")])])])]),e._v(" "),a("h3",{attrs:{id:"_12-27-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-27-2021"}},[e._v("#")]),e._v(" 12/27/2021")]),e._v(" "),a("ol",[a("li",[e._v("Go on reading papers")])]),e._v(" "),a("h3",{attrs:{id:"_12-28-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-28-2021"}},[e._v("#")]),e._v(" 12/28/2021")]),e._v(" "),a("ol",[a("li",[e._v("loss choice in ReVeSeg.\n"),a("blockquote",[a("p",[e._v("However, the objects in medical images such as optic disc and retinal vessels often occupy a small region in the image. The cross entropy loss is not optimal for such tasks. In this paper, we use the Dice coefficient loss function [57], [58] to replace the common cross entropy loss.")])])])]),e._v(" "),a("h3",{attrs:{id:"_12-30-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-30-2021"}},[e._v("#")]),e._v(" 12/30/2021")]),e._v(" "),a("p",[e._v("Wait! Spin!!! This seems still not really bad. Just ran another training for Spin. (1640837519, tmux7)\nSANet can segment to some extent, but we need to verify it a little bit. Untested for the latest one.")]),e._v(" "),a("ul",[a("li",[e._v("[X] modify the SANet -----\x3e no good"),a("br"),e._v("\nJust ran a UNet for comparison.")])]),e._v(" "),a("h3",{attrs:{id:"_12-31-2021"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_12-31-2021"}},[e._v("#")]),e._v(" 12/31/2021")]),e._v(" "),a("p",[e._v("Weird, why UNet won't work here?")]),e._v(" "),a("h3",{attrs:{id:"_01-01-2022"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_01-01-2022"}},[e._v("#")]),e._v(" 01/01/2022")]),e._v(" "),a("p",[e._v("Changed the loss function, then it works quite well."),a("br"),e._v("\nDoes this mean my modification on the SANet has little to no effect?... Oops...")]),e._v(" "),a("h3",{attrs:{id:"_01-03-2022"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_01-03-2022"}},[e._v("#")]),e._v(" 01/03/2022")]),e._v(" "),a("p",[e._v("Go on papers!")]),e._v(" "),a("h3",{attrs:{id:"_01-04-2022"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_01-04-2022"}},[e._v("#")]),e._v(" 01/04/2022")]),e._v(" "),a("ul",[a("li",[e._v("[X] Just transfered the SANet (Spatial Attention Network) from Keras to Pytorch.")]),e._v(" "),a("li",[e._v("[X] "),a("strong",[e._v("Untrained. No CPU yet.")])]),e._v(" "),a("li",[e._v("[X] Paper: EAR-NET: Error Attention Refining Network ForRetinal Vessel Segmentation "),a("a",{attrs:{href:"https://github.com/Markin-Wang/EARNet",target:"_blank",rel:"noopener noreferrer"}},[e._v("Github"),a("OutboundLink")],1)]),e._v(" "),a("li",[e._v("[X] Implement the famous apex")])]),e._v(" "),a("h3",{attrs:{id:"_01-05-2022"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_01-05-2022"}},[e._v("#")]),e._v(" 01/05/2022")]),e._v(" "),a("ul",[a("li",[e._v("[X] "),a("s",[e._v("Go on on the EAR-NET")])]),e._v(" "),a("li",[e._v("[ ] runx: an experiment managemer # TODO")]),e._v(" "),a("li",[e._v("[X] Just implemented the depth2space/depth2space to replace the pooling function. That's "),a("code",[e._v("SA_Net_v2")]),e._v(". Should I use any guidance to choose which neighbors just as in the paper (subpixel embedding?)")])]),e._v(" "),a("p",[e._v("$\\color{red}\\text{Status Quo}$:"),a("br"),e._v("\nBecause of no GPU for now:")]),e._v(" "),a("ol",[a("li",[e._v("SA_Net, the transfered Pytorch version from Keras one, remains un-verified. Does it still have the potential to reach the state-of-the-art?  ------\x3e "),a("code",[e._v("base")]),e._v(" (violance of naming ethnics)\n"),a("ul",[a("li",[e._v("[ ] Remained to be tested")])])]),e._v(" "),a("li",[e._v("SA_Net_v2, the replace-all-maxpooling-and-transposeconv-with-s2d-or-d2s version, remains un-verified. ------\x3e "),a("code",[e._v("sa2")]),e._v(" "),a("ul",[a("li",[e._v("[ ] Remained to be tested")])])]),e._v(" "),a("li",[e._v("SA_Net_v3, the not-replacement-but-guide-in-different-way version, remains un-verified.  -----\x3e "),a("code",[e._v("sa3")]),e._v(" "),a("ul",[a("li",[e._v("[ ] Remained to be tested")])])]),e._v(" "),a("li",[e._v("An Unet to check if everything works -----\x3e "),a("code",[e._v("unet")]),e._v(" yes, things do work the same way as before\n"),a("ul",[a("li",[e._v("[ ] Remained to be tested")])])]),e._v(" "),a("li",[e._v("A multiplication-fusion-mode version of SA_Net_v3 -----\x3e "),a("code",[e._v("sa3_mul")]),a("br"),e._v(" "),a("strong",[e._v("Result")]),e._v(":")]),e._v(" "),a("li",[e._v("They do decrese to some extent, but just too slow and vibrating. Or it's just the formal models tend to over-fit while they reflect the true thing?")])]),e._v(" "),a("h3",{attrs:{id:"_01-06-2022"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_01-06-2022"}},[e._v("#")]),e._v(" 01/06/2022")]),e._v(" "),a("p",[e._v("Why not just did everything on the UNet?")]),e._v(" "),a("ul",[a("li",[e._v("[ ]  Start to implement the same idea on the UNet. There, again, should be "),a("strong",[e._v("three")]),e._v(" versions.")]),e._v(" "),a("li",[e._v("two training processes are going on.\n"),a("ul",[a("li",[a("code",[e._v("/mnt/data3/zzx/Results/ReVeSeg/DRIVE_UNet_v3/1641460260/")])]),e._v(" "),a("li",[a("code",[e._v("/mnt/data3/zzx/Results/ReVeSeg/DRIVE_UNet_v2/1641460871/")])])])])]),e._v(" "),a("ol",[a("li",[e._v("The Dropout seems to be of no use. We'll see if the above two can bring etwas gut. Hoffentlich.")])]),e._v(" "),a("ul",[a("li",[e._v("[X] PPT.")])]),e._v(" "),a("h3",{attrs:{id:"_01-07-2022"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_01-07-2022"}},[e._v("#")]),e._v(" 01/07/2022")]),e._v(" "),a("ul",[a("li",[e._v("[ ] Learning rate issue remains to be solved. Should not always increase.")]),e._v(" "),a("li",[e._v("[ ] Pool the other way: Please immediately implement them to check if there is any difference, though I do think they perform the same way...\n"),a("ol",[a("li",[a("strong",[e._v("Strategy1:")])])]),e._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[e._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("Conv2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("in_channel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" out_channel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n")])])]),a("ol",{attrs:{start:"2"}},[a("li",[a("strong",[e._v("Strategy2:")])])]),e._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("class")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("DynamicPooling")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("def")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" channel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[e._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("s2d "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" SpaceToDepth"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v(" \n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("maxpool "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" Max_pool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v(" \n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("avgpool "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" Avg_pool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v(" \n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("concat "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" Concatenate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("conv "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" conv_block"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("2")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" channel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("4")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" channel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("def")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(":")]),e._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# import ipdb; ipdb.set_trace()")]),e._v("\n        deep_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("s2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# channel ----\x3e 4 * channel")]),e._v("\n        max_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("maxpool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("deep_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n        avg_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("avgpool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("deep_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n        con "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("concat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("max_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" avg_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("conv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("con"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n        weighted_x_deep "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" out "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" deep_x\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("avgpool"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("weighted_x_deep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("4")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# sum")]),e._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("return")]),e._v(" out\n")])])]),a("ol",{attrs:{start:"3"}},[a("li",[a("strong",[e._v("Search if any previous works")])])])]),e._v(" "),a("li",[e._v("[ ] And agian, how the s2d/d2s really works?")])]),e._v(" "),a("blockquote",[a("p",[e._v("Controversially, Geoff Hinton doesnâ€™t like Max-Pooling:\nThe pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.")])]),e._v(" "),a("h2",{attrs:{id:"paper-list"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#paper-list"}},[e._v("#")]),e._v(" Paper list")]),e._v(" "),a("p",[e._v("Auch, we have MICCAI21 paper list for "),a("strong",[e._v("medical image segmentation")]),e._v(".")]),e._v(" "),a("ol",[a("li",[e._v("A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation")]),e._v(" "),a("li",[e._v("Anatomy-Constrained Contrastive Learning for Synthetic Segmentation without Ground-truth")]),e._v(" "),a("li",[e._v("ASC-Net: Adversarial-based Selective Network for Unsupervised Anomaly Segmentation")]),e._v(" "),a("li",[a("ul",[a("li",[e._v("[x] Automatic Polyp Segmentation via Multi-scale Subtraction Network")])])]),e._v(" "),a("li",[a("ul",[a("li",[e._v("[x] CCBANet: Cascading Context and Balancing Attention for Polyp Segmentation")])])]),e._v(" "),a("li",[e._v("Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation")]),e._v(" "),a("li",[a("s",[e._v("HRENet: A Hard Region Enhancement Network for Polyp Segmentation")])]),e._v(" "),a("li",[a("s",[e._v("Hybrid graph convolutional neural networks for anatomical segmentation")])]),e._v(" "),a("li",[e._v("MT-UDA: Towards Unsupervised Cross-Modality Medical Image Segmentation with Limited Source Labels")]),e._v(" "),a("li",[a("s",[e._v("Multi-Compound Transformer for Accurate Biomedical Image Segmentation")])]),e._v(" "),a("li",[e._v("OLVA: Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation")]),e._v(" "),a("li",[a("s",[e._v("RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs using a Novel Multi-scale Generative Adversarial Network")])]),e._v(" "),a("li",[a("ul",[a("li",[e._v("[x] Shallow Attention Network for Polyp Segmentation")])])]),e._v(" "),a("li",[a("s",[e._v("Towards Robust General Medical Image Segmentation")])]),e._v(" "),a("li",[e._v("TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation")]),e._v(" "),a("li",[e._v("Unsupervised Network Learning for Cell Segmentation")])]),e._v(" "),a("p",[e._v("MICCAI20 paper list for "),a("strong",[e._v("medical image segmentation")])]),e._v(" "),a("ol",[a("li",[a("s",[e._v("Source-Relaxed Domain Adaptation for Image Segmentation")])]),e._v(" "),a("li",[e._v("iU-Net: Towards Accurate Segmentation of Biomedical Images Using Over-Complete Representations")]),e._v(" "),a("li",[e._v("Self-supervised Nuclei Segmentation in Histopathological Images Using Attention")]),e._v(" "),a("li",[e._v("Adaptive Context Selection for Polyp Segmentation")]),e._v(" "),a("li",[e._v("PraNet: Parallel Reverse Attention Network for Polyp Segmentation")]),e._v(" "),a("li",[e._v("White Matter Tract Segmentation with Self-supervised Learning")])]),e._v(" "),a("p",[e._v("MICCAI19 paper list for "),a("strong",[e._v("medical image segmentation")])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("1. Boundary and Entropy-Driven Adversarial Learning for Fundus Image Segmentation\n2. Selective Feature Aggregation Network with Area-Boundary Constraints for Polyp Segmentation\n3. ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation\n4. PHiSeg: Capturing Uncertainty in Medical Image Segmentation\n5. Constrained Domain Adaptation for Segmentation\n6. Refined Segmentation R-CNN: A Two-Stage Convolutional Neural Network for Punctate White Matter Lesion Segmentation in Preterm Infants\n")])])]),a("p",[e._v("Others:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("1. Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging\n2. Patch-Free 3D Medical Image Segmentation Driven by Super-Resolution Technique and Self-Supervised Guidance\n3. Self-supervised visual representation learning for histopathological images\n4. Self-Supervised Vessel Enhancement Using Flow-Based Consistencies\nNot finished...\n")])])]),a("hr"),e._v(" "),a("h2",{attrs:{id:"paper-note"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#paper-note"}},[e._v("#")]),e._v(" Paper note")]),e._v(" "),a("details",[a("summary",[e._v("Automatic Polyp Segmentation via Multi-scale Subtraction Network")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight: (https://github.com/Xiaoqi-Zhao-DLUT/MSNet)\n    ")]),a("p",[e._v("1. multi-scale subtraction network with multi-level and multi-stage cascaded subtraction operations\n    ")]),e._v(" "),a("p",[e._v("2. LossNet. a training-free loss network to implement detail-to-structure supervision\n")])]),e._v(" "),a("hr"),e._v(" "),a("details",[a("summary",[e._v("CCBANet: Cascading Context and Balancing Attention for Polyp Segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight: (https://github.com/ntcongvn/CCBANet)\n    ")]),a("p",[e._v("1. Balancing Attention Module. The BAM module implements separately and balances the attention mechanism for three regions: background, foreground, and the boundary site.\n    ")]),e._v("\nThought:\n    "),a("p",[e._v(" 1. Should we consider the background attention? It seems quite important to cover this in the attention mechanism, and it has valid implementation background. Seemingly a fairly well module in the decoder?\n    ")]),e._v(" "),a("p",[e._v(" 2. The boundary seems quite tricky, isn't that using a simple calculation too simple? Ich weiss es nicht. \n    ")]),e._v(" "),a("p",[e._v(" 3. MIX IT UP! What if there is only one module in the skip connection path, based on the fact that I change the backbone to transformer? (Long range issue resolved inherently) \n    ")]),a("p")]),e._v(" "),a("hr"),e._v(" "),a("details",[a("summary",[e._v("Adaptive Context Selection for Polyp Segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight: (https://github.com/ReaFly/ACSNet)\n    ")]),a("p",[e._v("1. Local Context Attention Module (LCA). A kind of spatial attention scheme which attends more on the `local` part.\n    ")]),e._v(" "),a("p",[e._v("2. Global Context Module (GCM). Four branches to extract features at different scales, capturing the long-range dependency while maintaining the original resolution.\n    ")]),e._v(" "),a("p",[e._v("3. Adaptive Selection Module (ASM). Aiming to adaptively select context feature for aggregation in each block. Plus incorporation with a SE block to adaptively recalibrate channel-wise feature responses for feature selection.\n    ")]),e._v("\nThought:\n    "),a("p",[e._v(" 1. Can I use SwinTransformer as the backbone, and then focus on the local context feature in decoder?\n    ")]),e._v(" "),a("p",[e._v(" 2. There seems to be a multi-level manner in the decoder? \n    ")]),e._v(" "),a("p")]),e._v(" "),a("hr"),e._v(" "),a("details",[a("summary",[e._v("Shallow Attention Network for Polyp Segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nIssue overhead:\n    ")]),a("p",[e._v(" 1. inconsistent colors among samples\n    ")]),e._v(" "),a("p",[e._v(" 2. feature downsampling leads to degraded small polyps\n    ")]),e._v(" "),a("p",[e._v(" 3. foreground and background pixels are imbalanced, leading to biased training\n    ")]),e._v("\nInsight: (https://github.com/weijun88/SANet)\n    "),a("p",[e._v(" 1. to enhance the segmentation quality of small polyps, shallow attention module is used to filter out the background noise of shallow features (What is shallow features?)  \n    ")]),e._v(" "),a("p",[e._v(" 2. to erase the severe pixel imbalance for small polyps, a probability correction strategy is proposed\n    ")]),e._v(" "),a("p",[e._v(" 3. small polyp segmentation relies more on shallow features, because they have higher resolutions and contain richer details compared with deep ones.  But the shallow features are too noisy to be used directly.\n    ")]),e._v(" "),a("p",[e._v(" 4. "),a("font",{attrs:{color:"red"}},[a("b",[e._v("PCS")])])],1),e._v("\nThought:\n    "),a("p",[e._v(" 1. In retinal vessel segmentation task also exists such issues. Can we transfer that to the problem? inconsistent colors, imbalanced ratio of marco and micro vessels.\n    ")]),e._v(" "),a("p",[e._v(" 2. ðŸ™‚ The rationale behind the Insight3 also holds true for ReVe segmentation task. \n    ")]),e._v(" "),a("p",[e._v(" 3. "),a("b",[e._v("Enhancement of features")])]),e._v(" "),a("p")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("def save_prediction(self):\n        print(self.args.datapath.split('/')[-1])\n        with torch.no_grad():\n            for image, mask, shape, name, origin in self.loader:\n                image = image.cuda().float()\n                pred  = self.model(image)\n                pred  = F.interpolate(pred, size=shape, mode='bilinear', align_corners=True)[0, 0]\n                pred[torch.where(pred>0)] /= (pred>0).float().mean()\n                pred[torch.where(pred<0)] /= (pred<0).float().mean()\n                pred  = torch.sigmoid(pred).cpu().numpy()*255\n                if not os.path.exists(self.args.predpath):\n                    os.makedirs(self.args.predpath)\n                cv2.imwrite(self.args.predpath+'/'+name[0], np.round(pred))\n")])])]),a("details",[a("summary",[e._v("KiU-Net: Towards Accurate Segmentation of Biomedical Images Using Over-Complete Representations")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight:\n")]),a("p",[e._v(" 1. The increasing receptive field reasons is critical for CNNs to learn high-level features like objects, shapes or blobs. However, a side effect is that it reduces the focus of the filters. That is, except the first layer, filters in the other layers have reduced abilities to learn features that correspond to fine details like edges and their texture, which causes any network with the standard under-complete architecture to not produce sharp perdictions around the edges in tasks like segmentation.\n")]),e._v(" "),a("p",[e._v(" 2. Hence, by constricting the receptive field size, filters in the deep layers can still learn very fine edges as it tries to focus heavily on smaller regions.\n")]),e._v(" "),a("p",[e._v(" 3. Cross Resudual Fusion Module. One can perform simple concatenation of features at the final layer, however, this may not be necessarily optimal. Instead, feature maps are combined at each block, which results in better convergence as the flow of gradients during back propagation is across both the branches at each block level.")]),e._v("\nThought:\n"),a("p",[e._v(" 1. See the Insight1? In the MICCAI21, we can see that the SANet attempts to address such issue by introducing the shallow attention module. Especially for retinal vessel segmentation, we can say that fine details such as edges, mirco-vessels are foremost important for ReVeSeg.")]),e._v(" "),a("p",[e._v(" 2. Auch, local, local and local!")]),e._v(" "),a("p")]),e._v(" "),a("details",[a("summary",[e._v("Small Lesion Segmentation in Brain MRIs with Subpixel Embedding")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight:\n")]),a("p",[e._v(" 1. Subpixel embedding. We proposed to retain small local structures by learning an "),a("b",[e._v("embedding")]),e._v(" that maps the input to high dimensional feature maps of twice the input resolution. Unlike typical CNNs, such operation do not perform lossy downsampling on the representation, hence, "),a("b",[e._v("the embedding preserves local structures, but lacks global context")]),e._v(".")]),e._v(" "),a("p")]),e._v(" "),a("details",[a("summary",[e._v("Joint Segment-level and Pixel-Wise Losses for Deep Learning Based Retinal Vessel Segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight:\n")]),a("p",[e._v(" 1. Deep learning based methods for retinal vessel segmentation are usually trained based on pixel-wise losses, which treat all vessel pixels with equal importance in pixel-to-pixel matching between a predicted probability map and the corresponding mannually annotated segmentation. However, due to the highly imbalanced pixel ratio between thick and thin vessels in fundus images, a pixel-wise loss would limit deep learning models to learn features for accurate segmentation of thin vessels. (exactly what I thought backwards) ")]),e._v(" "),a("p",[e._v(" 2. Greater thickness inconsistency of thin vessels is due to the dominant amount of thick vessel pixels which makes the optimization of segmentation results largely influenced by the thick vessels.")]),e._v(" "),a("p",[e._v(" 3. threshold-free vessel segmentation\nwe apply the Otsu algorithm to automatically convert the probability map into the hard segmentation map.\n")]),e._v("\nThought:\n"),a("p",[e._v(" 1. What about thinness attention? local attention? ")]),e._v(" "),a("p",[e._v(" 2. Can hard threshold be soft? Yes.")]),e._v(" "),a("p",[e._v(" 3. ")]),e._v(" "),a("p")]),e._v(" "),a("details",[a("summary",[e._v("NFN+: A novel network followed network for retinal vessel segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight:\n")]),a("p",[e._v(" 1. The difficulties are mainly caused by intensity inhomogeneity, low contrast, and variable thickness between capillaries and major vessels on a fundus retinal image. The retinal lesions and exudates can further complicate the segmentation.")]),e._v(" "),a("p",[e._v(" 2. Inter-vessel differences. Although the pooling layer used in DCNNs decreases the computational complexity, the down-sampling process would remove the fine structures such as capillaries, which can hardly be recovered by the up-sampling layers in the decoder.")]),e._v(" "),a("p",[e._v(" 3. Lack of structured prediction. Although DCNNs have a strong ability to learn image representations, they can hardly incorporate the spatial information of target regions into the unified image representations and pixel classification architecture. ...there is still an inherent contradictory between the pixel-level DCNNs and semantic-level structured prediction tasks. ")]),e._v(" "),a("p",[e._v(" 4. The front network converts an image patch into a probalistic retinal vessel map, and the followed network further refines the map into a segmentation result. \n")]),e._v(" "),a("p",[e._v(" 5. Similar to giving those students with poor grades a chance to review and have a make-up examination, we use the followed network to learn the discrimination of vessels and background again based on the features and predictions produced by the front network, to correct some of those mis-classified pixels, particularly those with a probability close to the cut-off.  \n")]),e._v(" "),a("p",[e._v(" 6. The image segmentation process can be regarded as a transfer route from the original image domain to the segmentation target domain. Such abstractions, however, may cause the loss of useful semantic and spatial information stage by stage. (One way is to use skip connection, another way is to use other down-sampling operations, such as space2depth and depth2space?) \n")]),e._v("\nThought:\n"),a("p",[e._v(" 1. What exactly is "),a("b",[e._v("multi-scale")])]),e._v(" "),a("p",[e._v(" 2. coarse-to-fine processing and gradual abstraction")]),e._v(" "),a("p",[e._v(" 3. Overall, this paper offer us information about what the challenges are and why the challenges happen. ")]),e._v(" "),a("p")]),e._v(" "),a("details",[a("summary",[e._v("SCS-Net: A Scale and Context Sensitive Network for Retinal Vessel Segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight:\n")]),a("p",[e._v(" 1. An intuitive idea is that the semantic information in the high-level features and the spatial information in the low-level features can be fully combined while suppressing mimics and noise.")]),e._v(" "),a("p",[e._v(" 2. ")]),e._v(" "),a("p",[e._v(" 3.")]),e._v("\nThought:\n"),a("p",[e._v(" 1.")]),e._v(" "),a("p",[e._v(" 2.")]),e._v(" "),a("p",[e._v(" 3.")]),e._v(" "),a("p")]),e._v(" "),a("details",[a("summary",[e._v("EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation")]),e._v(" "),a("p",{attrs:{align:"left"}},[e._v("\nInsight:\n")]),a("p",[e._v(" 1. One major problem in the deep learning based models defecting the retinal blood vessel segmentation performance is that they normally recover the high-resolution representation from the low-resolution features. Essential information may be lost during this phase, causing inaccurate results.")]),e._v(" "),a("p",[e._v(" 2. Besides, most existing methods struggle to handle the pixel around the brighter and darker spots, leading to the false prediction.")]),e._v(" "),a("p",[e._v(" 3. "),a("b",[e._v("Moreover, most of existing works are dominated by the background due to the significant imbalance between the foreground and background, leading to high accuracy and specificity but low sensitivity.")])]),e._v("\nThought:\n"),a("p",[e._v(" 1.")]),e._v(" "),a("p",[e._v(" 2.")]),e._v(" "),a("p",[e._v(" 3.")]),e._v(" "),a("p")])])}),[],!1,null,null,null);t.default=s.exports}}]);